# kintsugi_ava/core/plugins/examples/system_integrator/__init__.py
# System Integrator Plugin - Production-quality holistic code integration analysis and fixing

import asyncio
import ast
import json
import re
from pathlib import Path
from typing import Dict, Any, List, Set, Optional, Tuple
from datetime import datetime
from collections import defaultdict
from dataclasses import dataclass

from core.plugins import PluginBase, PluginMetadata, PluginState


@dataclass
class IntegrationIssue:
    """Represents a code integration issue that needs fixing."""
    issue_type: str
    severity: str  # "critical", "high", "medium", "low"
    file_path: str
    line_number: Optional[int]
    description: str
    context: Dict[str, Any]
    suggested_fix: Optional[str] = None
    fix_confidence: float = 0.0  # 0.0 to 1.0


@dataclass
class CodeElement:
    """Represents a code element (class, function, method) with its signature."""
    name: str
    element_type: str  # "class", "function", "method"
    file_path: str
    line_number: int
    signature: Dict[str, Any]
    dependencies: Set[str]
    parent_class: Optional[str] = None


class SystemIntegratorPlugin(PluginBase):
    """
    Production-quality System Integrator that ensures all generated code works together.

    This plugin:
    - Performs deep AST analysis of the entire codebase
    - Identifies and fixes integration issues between modules
    - Validates constructor calls, method signatures, and interfaces
    - Ensures main.py properly initializes all components
    - Applies LLM-powered fixes for complex integration problems
    - Integrates fully with ProjectManager and LLMClient
    """

    def __init__(self, event_bus, plugin_config: Dict[str, Any]):
        super().__init__(event_bus, plugin_config)

        # Service references (injected by ServiceManager)
        self.service_manager = None
        self.project_manager = None
        self.llm_client = None

        # Code analysis state
        self.code_elements: Dict[str, CodeElement] = {}
        self.integration_issues: List[IntegrationIssue] = []
        self.import_map: Dict[str, Set[str]] = defaultdict(set)  # file -> imported names
        self.definition_map: Dict[str, str] = {}  # name -> file where defined
        self.usage_map: Dict[str, List[Tuple[str, int]]] = defaultdict(list)  # name -> [(file, line)]

        # Integration tracking
        self.last_integration_check: Optional[datetime] = None
        self.fixes_applied: List[Dict[str, Any]] = []
        self.integration_queue: asyncio.Queue = asyncio.Queue()

        # Processing state
        self.is_analyzing = False
        self.integration_worker_task: Optional[asyncio.Task] = None

    @property
    def metadata(self) -> PluginMetadata:
        return PluginMetadata(
            name="system_integrator",
            version="1.0.0",
            description="Production-quality system integration analysis and automatic fixing",
            author="Kintsugi AvA Team",
            dependencies=[],
            event_subscriptions=[
                "code_generation_complete",
                "prepare_for_generation",
                "execution_failed",
                "new_project_requested"
            ],
            event_emissions=[
                "integration_analysis_started",
                "integration_issue_detected",
                "integration_fix_applied",
                "integration_analysis_complete"
            ],
            config_schema={
                "auto_integration_check": {
                    "type": "bool",
                    "default": True,
                    "description": "Automatically check integration after code generation"
                },
                "auto_fix_enabled": {
                    "type": "bool",
                    "default": True,
                    "description": "Automatically fix detected integration issues"
                },
                "fix_confidence_threshold": {
                    "type": "float",
                    "default": 0.8,
                    "description": "Minimum confidence level to auto-apply fixes (0.0-1.0)"
                },
                "analysis_depth": {
                    "type": "str",
                    "default": "deep",
                    "description": "Analysis depth: 'quick', 'standard', 'deep'"
                },
                "detailed_logging": {
                    "type": "bool",
                    "default": True,
                    "description": "Log detailed integration analysis information"
                }
            },
            enabled_by_default=True
        )

    async def load(self) -> bool:
        try:
            self.log("info", "Loading System Integrator...")
            self._reset_analysis_state()
            self.set_state(PluginState.LOADED)
            self.log("success", "System Integrator loaded")
            return True
        except Exception as e:
            self.log("error", f"Failed to load System Integrator: {e}")
            self.set_state(PluginState.ERROR)
            return False

    async def start(self) -> bool:
        try:
            self.log("info", "Starting System Integrator...")

            # Get service references
            await self._initialize_service_references()

            # Subscribe to events
            self.subscribe_to_event("code_generation_complete", self._on_code_generated)
            self.subscribe_to_event("prepare_for_generation", self._on_generation_prepared)
            self.subscribe_to_event("execution_failed", self._on_execution_failed)
            self.subscribe_to_event("new_project_requested", self._on_new_project)

            # Start integration worker
            self.integration_worker_task = asyncio.create_task(self._integration_worker())

            self.set_state(PluginState.STARTED)
            self.log("info", "ðŸ”— System Integrator active - production-quality integration monitoring")
            return True

        except Exception as e:
            self.log("error", f"Failed to start System Integrator: {e}")
            self.set_state(PluginState.ERROR)
            return False

    async def stop(self) -> bool:
        try:
            self.log("info", "Stopping System Integrator...")

            if self.integration_worker_task:
                self.integration_worker_task.cancel()
                try:
                    await self.integration_worker_task
                except asyncio.CancelledError:
                    pass

            await self._generate_integration_report()
            self.set_state(PluginState.STOPPED)
            return True

        except Exception as e:
            self.log("error", f"Failed to stop System Integrator: {e}")
            self.set_state(PluginState.ERROR)
            return False

    async def unload(self) -> bool:
        try:
            self.log("info", "Unloading System Integrator...")
            self._reset_analysis_state()
            self.set_state(PluginState.UNLOADED)
            self.log("info", "System Integrator unloaded")
            return True
        except Exception as e:
            self.log("error", f"Failed to unload System Integrator: {e}")
            self.set_state(PluginState.ERROR)
            return False

    async def _initialize_service_references(self):
        """Initialize references to required services."""
        # Wait a bit for services to be fully initialized
        await asyncio.sleep(0.1)

        # Get service manager from event bus context (this would be injected in a real implementation)
        # For now, we'll emit an event to request service manager reference
        self.emit_event("plugin_service_manager_request", "system_integrator")

    def set_service_manager(self, service_manager):
        """Called by ServiceManager to inject the reference."""
        self.service_manager = service_manager
        self.project_manager = service_manager.get_project_manager()
        self.llm_client = service_manager.get_llm_client()
        self.log("info", "Service references initialized")

    # Event Handlers
    def _on_code_generated(self, files: Dict[str, str]):
        """Handle code generation completion - queue integration analysis."""
        if not self.get_config_value("auto_integration_check", True):
            return

        self.log("info", f"ðŸ” Code generation complete - queueing deep integration analysis for {len(files)} files")

        asyncio.create_task(self.integration_queue.put({
            "type": "full_analysis",
            "files": files,
            "timestamp": datetime.now().isoformat(),
            "priority": "normal"
        }))

    def _on_generation_prepared(self, filenames: List[str], project_path: str = None):
        """Handle generation preparation."""
        if self.get_config_value("detailed_logging", True):
            self.log("info", f"ðŸ”„ Generation prepared for {len(filenames)} files - resetting analysis state")
        self._reset_analysis_state()

    def _on_execution_failed(self, error_report: str):
        """Handle execution failures - high priority integration analysis."""
        self.log("warning", "ðŸ’¥ Execution failed - prioritizing integration analysis")

        asyncio.create_task(self.integration_queue.put({
            "type": "error_focused_analysis",
            "error_report": error_report,
            "priority": "critical",
            "timestamp": datetime.now().isoformat()
        }))

    def _on_new_project(self):
        """Handle new project creation."""
        self.log("info", "ðŸ†• New project detected - resetting integration state")
        self._reset_analysis_state()

    # Core Integration Analysis
    async def _integration_worker(self):
        """Worker that processes integration analysis requests."""
        self.log("info", "ðŸ¤– Integration worker started")

        while self.state == PluginState.STARTED:
            try:
                request = await asyncio.wait_for(self.integration_queue.get(), timeout=1.0)

                self.is_analyzing = True
                await self._process_integration_request(request)
                self.is_analyzing = False

            except asyncio.TimeoutError:
                continue
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.log("error", f"Error in integration worker: {e}")
                self.is_analyzing = False

    async def _process_integration_request(self, request: Dict[str, Any]):
        """Process integration analysis request with full error handling."""
        try:
            request_type = request.get("type", "unknown")
            priority = request.get("priority", "normal")

            self.log("info", f"ðŸ”— Processing {priority} integration request: {request_type}")

            self.emit_event("integration_analysis_started", {
                "request_type": request_type,
                "priority": priority,
                "timestamp": datetime.now().isoformat()
            })

            if request_type == "full_analysis":
                await self._perform_full_integration_analysis(request.get("files", {}))
            elif request_type == "error_focused_analysis":
                await self._perform_error_focused_analysis(request.get("error_report", ""))

            self.emit_event("integration_analysis_complete", {
                "request_type": request_type,
                "issues_found": len(self.integration_issues),
                "fixes_applied": len(self.fixes_applied),
                "timestamp": datetime.now().isoformat()
            })

        except Exception as e:
            self.log("error", f"Failed to process integration request: {e}")

    async def _perform_full_integration_analysis(self, generated_files: Dict[str, str]):
        """Perform comprehensive integration analysis."""
        try:
            self.log("info", "ðŸ” Starting comprehensive integration analysis...")

            # Get all project files, not just generated ones
            all_project_files = await self._get_all_project_files()

            # Merge with generated files (generated files take precedence)
            all_project_files.update(generated_files)

            if not all_project_files:
                self.log("warning", "No project files found for analysis")
                return

            self.log("info", f"ðŸ“Š Analyzing {len(all_project_files)} total files")

            # Step 1: Deep AST analysis of all files
            await self._build_comprehensive_code_map(all_project_files)

            # Step 2: Identify integration issues
            issues = await self._identify_all_integration_issues()

            if issues:
                self.log("warning", f"âš ï¸ Found {len(issues)} integration issues")

                # Categorize by severity
                critical = [i for i in issues if i.severity == "critical"]
                high = [i for i in issues if i.severity == "high"]
                medium = [i for i in issues if i.severity == "medium"]
                low = [i for i in issues if i.severity == "low"]

                self.log("info",
                         f"ðŸ“Š Issues by severity - Critical: {len(critical)}, High: {len(high)}, Medium: {len(medium)}, Low: {len(low)}")

                # Step 3: Apply fixes if enabled
                if self.get_config_value("auto_fix_enabled", True):
                    await self._apply_comprehensive_fixes(issues)
            else:
                self.log("success", "âœ… No integration issues found - project is well integrated")

            self.last_integration_check = datetime.now()

        except Exception as e:
            self.log("error", f"Failed to perform integration analysis: {e}")
            import traceback
            traceback.print_exc()

    async def _get_all_project_files(self) -> Dict[str, str]:
        """Get all Python files from the current project."""
        if not self.project_manager or not self.project_manager.active_project_path:
            return {}

        try:
            project_files = {}
            project_root = self.project_manager.active_project_path

            # Get all Python files recursively
            for py_file in project_root.rglob("*.py"):
                # Skip venv and cache directories
                if any(skip in py_file.parts for skip in ['.venv', '__pycache__', '.git']):
                    continue

                try:
                    relative_path = py_file.relative_to(project_root)
                    content = py_file.read_text(encoding='utf-8')
                    project_files[str(relative_path).replace('\\', '/')] = content
                except Exception as e:
                    self.log("warning", f"Could not read {py_file}: {e}")

            self.log("info", f"ðŸ“ Found {len(project_files)} Python files in project")
            return project_files

        except Exception as e:
            self.log("error", f"Failed to get project files: {e}")
            return {}

    async def _build_comprehensive_code_map(self, files: Dict[str, str]):
        """Build comprehensive map of all code elements, imports, and usage."""
        try:
            self.log("info", "ðŸ—ºï¸ Building comprehensive code map...")

            # Clear previous analysis
            self.code_elements.clear()
            self.import_map.clear()
            self.definition_map.clear()
            self.usage_map.clear()

            # Analyze each file
            for file_path, content in files.items():
                if file_path.endswith('.py'):
                    await self._analyze_file_comprehensively(file_path, content)

            # Build cross-references
            self._build_cross_references()

            self.log("success",
                     f"ðŸ“‹ Code map complete: {len(self.code_elements)} elements, {len(self.definition_map)} definitions")

        except Exception as e:
            self.log("error", f"Failed to build code map: {e}")

    async def _analyze_file_comprehensively(self, file_path: str, content: str):
        """Perform comprehensive analysis of a single file."""
        try:
            tree = ast.parse(content)

            # Track imports
            self._extract_imports(tree, file_path)

            # Track definitions and usage
            visitor = ComprehensiveASTVisitor(file_path)
            visitor.visit(tree)

            # Store results
            for element in visitor.elements:
                key = f"{file_path}::{element.name}"
                self.code_elements[key] = element
                self.definition_map[element.name] = file_path

            # Store usage information
            for name, line in visitor.name_usage:
                self.usage_map[name].append((file_path, line))

        except SyntaxError as e:
            # Syntax errors are critical integration issues
            issue = IntegrationIssue(
                issue_type="syntax_error",
                severity="critical",
                file_path=file_path,
                line_number=e.lineno,
                description=f"Syntax error: {e.msg}",
                context={"error": str(e), "text": e.text},
                fix_confidence=0.9
            )
            self.integration_issues.append(issue)
        except Exception as e:
            self.log("warning", f"Could not analyze {file_path}: {e}")

    def _extract_imports(self, tree: ast.AST, file_path: str):
        """Extract all imports from a file."""
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imported_name = alias.asname or alias.name
                    self.import_map[file_path].add(imported_name)
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    for alias in node.names:
                        imported_name = alias.asname or alias.name
                        self.import_map[file_path].add(imported_name)

    def _build_cross_references(self):
        """Build cross-references between definitions and usage."""
        for element_key, element in self.code_elements.items():
            # Add imports as dependencies
            file_imports = self.import_map.get(element.file_path, set())
            element.dependencies.update(file_imports)

    async def _identify_all_integration_issues(self) -> List[IntegrationIssue]:
        """Identify all types of integration issues."""
        issues = []

        # Check for undefined name usage
        issues.extend(await self._check_undefined_names())

        # Check constructor calls
        issues.extend(await self._check_constructor_calls())

        # Check function calls
        issues.extend(await self._check_function_calls())

        # Check main.py integration
        issues.extend(await self._check_main_py_integration())

        # Check import consistency
        issues.extend(await self._check_import_consistency())

        # Store and emit issues
        self.integration_issues.extend(issues)

        for issue in issues:
            self.emit_event("integration_issue_detected", {
                "issue_type": issue.issue_type,
                "severity": issue.severity,
                "file_path": issue.file_path,
                "description": issue.description,
                "confidence": issue.fix_confidence
            })

        return issues

    async def _check_undefined_names(self) -> List[IntegrationIssue]:
        """Check for usage of undefined names."""
        issues = []

        for name, usage_locations in self.usage_map.items():
            # Skip built-ins and common names
            if name in {'self', 'cls', 'super', 'len', 'str', 'int', 'float', 'bool', 'list', 'dict', 'set', 'tuple'}:
                continue

            for file_path, line_num in usage_locations:
                # Check if name is defined in this file or imported
                file_imports = self.import_map.get(file_path, set())
                local_definitions = {elem.name for elem in self.code_elements.values() if elem.file_path == file_path}

                if name not in file_imports and name not in local_definitions and name not in self.definition_map:
                    issue = IntegrationIssue(
                        issue_type="undefined_name",
                        severity="high",
                        file_path=file_path,
                        line_number=line_num,
                        description=f"Undefined name '{name}' used",
                        context={"name": name, "available_definitions": list(self.definition_map.keys())},
                        fix_confidence=0.7
                    )
                    issues.append(issue)

        return issues

    async def _check_constructor_calls(self) -> List[IntegrationIssue]:
        """Check constructor calls against class definitions."""
        issues = []

        # This would require more sophisticated AST analysis to find call sites
        # For now, focus on main.py
        main_py_content = await self._get_file_content("main.py")
        if main_py_content:
            try:
                tree = ast.parse(main_py_content)
                for node in ast.walk(tree):
                    if isinstance(node, ast.Call) and isinstance(node.func, ast.Name):
                        class_name = node.func.id

                        # Find class definition
                        class_element = self._find_class_definition(class_name)
                        if class_element:
                            expected_args = class_element.signature.get("constructor_args", [])
                            provided_args = len(node.args)

                            if provided_args != len(expected_args):
                                issue = IntegrationIssue(
                                    issue_type="constructor_mismatch",
                                    severity="high",
                                    file_path="main.py",
                                    line_number=node.lineno,
                                    description=f"Constructor for {class_name} expects {len(expected_args)} args but got {provided_args}",
                                    context={
                                        "class_name": class_name,
                                        "expected_args": expected_args,
                                        "provided_args": provided_args,
                                        "definition_file": class_element.file_path
                                    },
                                    fix_confidence=0.9
                                )
                                issues.append(issue)
            except Exception as e:
                self.log("warning", f"Could not analyze main.py for constructor calls: {e}")

        return issues

    async def _check_function_calls(self) -> List[IntegrationIssue]:
        """Check function calls against definitions."""
        # This would require call-site analysis - placeholder for now
        return []

    async def _check_main_py_integration(self) -> List[IntegrationIssue]:
        """Check main.py for integration issues."""
        issues = []

        main_content = await self._get_file_content("main.py")
        if not main_content:
            return issues

        try:
            tree = ast.parse(main_content)

            # Check if main.py has proper error handling
            has_try_except = any(isinstance(node, ast.Try) for node in ast.walk(tree))
            if not has_try_except:
                issue = IntegrationIssue(
                    issue_type="missing_error_handling",
                    severity="medium",
                    file_path="main.py",
                    line_number=1,
                    description="main.py lacks error handling - could fail silently",
                    context={"suggestion": "Add try-except blocks around main logic"},
                    fix_confidence=0.8
                )
                issues.append(issue)

        except Exception as e:
            self.log("warning", f"Could not analyze main.py integration: {e}")

        return issues

    async def _check_import_consistency(self) -> List[IntegrationIssue]:
        """Check for import-related issues."""
        issues = []

        for file_path, imports in self.import_map.items():
            for imported_name in imports:
                # Check if imported name is actually defined somewhere in the project
                if imported_name not in self.definition_map and not self._is_external_import(imported_name):
                    issue = IntegrationIssue(
                        issue_type="missing_import_target",
                        severity="medium",
                        file_path=file_path,
                        line_number=1,  # Would need more analysis to get exact line
                        description=f"Import '{imported_name}' not found in project",
                        context={"import_name": imported_name},
                        fix_confidence=0.6
                    )
                    issues.append(issue)

        return issues

    def _is_external_import(self, name: str) -> bool:
        """Check if an import is external (standard library or third-party)."""
        external_prefixes = {
            'os', 'sys', 'json', 'ast', 're', 'pathlib', 'typing', 'datetime', 'collections',
            'asyncio', 'dataclasses', 'PySide6', 'qtawesome', 'aiohttp', 'openai'
        }
        return any(name.startswith(prefix) for prefix in external_prefixes)

    def _find_class_definition(self, class_name: str) -> Optional[CodeElement]:
        """Find class definition by name."""
        for element in self.code_elements.values():
            if element.element_type == "class" and element.name == class_name:
                return element
        return None

    async def _get_file_content(self, file_path: str) -> Optional[str]:
        """Get content of a specific file from the project."""
        if not self.project_manager or not self.project_manager.active_project_path:
            return None

        try:
            full_path = self.project_manager.active_project_path / file_path
            if full_path.exists():
                return full_path.read_text(encoding='utf-8')
        except Exception as e:
            self.log("warning", f"Could not read {file_path}: {e}")

        return None

    async def _apply_comprehensive_fixes(self, issues: List[IntegrationIssue]):
        """Apply fixes with confidence thresholding."""
        confidence_threshold = self.get_config_value("fix_confidence_threshold", 0.8)

        # Sort by severity and confidence
        issues_to_fix = sorted(
            [i for i in issues if i.fix_confidence >= confidence_threshold],
            key=lambda x: ({"critical": 0, "high": 1, "medium": 2, "low": 3}[x.severity], -x.fix_confidence)
        )

        self.log("info", f"ðŸ”§ Applying {len(issues_to_fix)} high-confidence fixes (threshold: {confidence_threshold})")

        files_to_update = {}

        for issue in issues_to_fix:
            fix_result = await self._generate_and_apply_fix(issue)
            if fix_result:
                if issue.file_path not in files_to_update:
                    files_to_update[issue.file_path] = await self._get_file_content(issue.file_path) or ""

                files_to_update[issue.file_path] = fix_result["new_content"]

                self.fixes_applied.append({
                    "issue": issue,
                    "fix_applied": fix_result["fix_description"],
                    "timestamp": datetime.now().isoformat(),
                    "confidence": issue.fix_confidence
                })

        # Apply all fixes at once
        if files_to_update and self.project_manager:
            try:
                commit_message = f"fix: system integration fixes - {len(self.fixes_applied)} issues resolved"
                self.project_manager.save_and_commit_files(files_to_update, commit_message)
                self.log("success", f"âœ… Applied {len(self.fixes_applied)} integration fixes and committed changes")
            except Exception as e:
                self.log("error", f"Failed to commit fixes: {e}")

    async def _generate_and_apply_fix(self, issue: IntegrationIssue) -> Optional[Dict[str, Any]]:
        """Generate and apply a fix for a specific issue."""
        try:
            if issue.issue_type == "undefined_name":
                return await self._fix_undefined_name(issue)
            elif issue.issue_type == "constructor_mismatch":
                return await self._fix_constructor_mismatch(issue)
            elif issue.issue_type == "syntax_error":
                return await self._fix_syntax_error(issue)
            elif issue.issue_type == "missing_error_handling":
                return await self._fix_missing_error_handling(issue)
            else:
                self.log("warning", f"No fix handler for issue type: {issue.issue_type}")
                return None

        except Exception as e:
            self.log("error", f"Failed to generate fix for {issue.issue_type}: {e}")
            return None

    async def _fix_undefined_name(self, issue: IntegrationIssue) -> Optional[Dict[str, Any]]:
        """Fix undefined name by adding appropriate import."""
        name = issue.context["name"]

        # Find where this name is defined
        if name in self.definition_map:
            definition_file = self.definition_map[name]

            # Generate import statement
            module_name = definition_file.replace('.py', '').replace('/', '.')
            import_statement = f"from {module_name} import {name}"

            # Get current file content
            current_content = await self._get_file_content(issue.file_path)
            if current_content:
                # Insert import at the top (after existing imports)
                lines = current_content.split('\n')
                import_insert_line = 0

                # Find where to insert the import
                for i, line in enumerate(lines):
                    if line.strip().startswith(('import ', 'from ')):
                        import_insert_line = i + 1
                    elif line.strip() and not line.strip().startswith('#'):
                        break

                lines.insert(import_insert_line, import_statement)
                new_content = '\n'.join(lines)

                return {
                    "new_content": new_content,
                    "fix_description": f"Added import: {import_statement}"
                }

        return None

    async def _fix_constructor_mismatch(self, issue: IntegrationIssue) -> Optional[Dict[str, Any]]:
        """Fix constructor mismatch using LLM."""
        if not self.llm_client:
            return None

        try:
            context = issue.context
            class_name = context["class_name"]
            expected_args = context["expected_args"]

            # Generate LLM prompt for constructor fix
            prompt = f"""Fix this constructor call in main.py:

Current issue: {issue.description}

Class: {class_name}
Expected constructor arguments: {expected_args}

Generate the correct constructor call. Respond with only the corrected line of code.
Example: MyClass(arg1, arg2, arg3)
"""

            # Use LLM to generate fix
            provider, model = self.llm_client.get_model_for_role("reviewer")
            if provider and model:
                fix_response = ""
                async for chunk in self.llm_client.stream_chat(provider, model, prompt, "reviewer"):
                    fix_response += chunk

                fix_response = fix_response.strip()

                if fix_response and class_name in fix_response:
                    # Apply the fix to the file
                    current_content = await self._get_file_content(issue.file_path)
                    if current_content:
                        lines = current_content.split('\n')
                        if issue.line_number and issue.line_number <= len(lines):
                            # Replace the problematic line
                            old_line = lines[issue.line_number - 1]
                            # Find the constructor call and replace it
                            if class_name in old_line:
                                # Simple replacement - could be more sophisticated
                                new_line = re.sub(rf'{class_name}\([^)]*\)', fix_response, old_line)
                                lines[issue.line_number - 1] = new_line

                                return {
                                    "new_content": '\n'.join(lines),
                                    "fix_description": f"Fixed constructor call: {fix_response}"
                                }

        except Exception as e:
            self.log("error", f"Failed to generate LLM fix for constructor: {e}")

        return None

    async def _fix_syntax_error(self, issue: IntegrationIssue) -> Optional[Dict[str, Any]]:
        """Fix syntax error using LLM."""
        if not self.llm_client:
            return None

        try:
            prompt = f"""Fix this Python syntax error:

File: {issue.file_path}
Line: {issue.line_number}
Error: {issue.description}
Problematic text: {issue.context.get('text', '')}

Provide the corrected line of code. Respond with only the fixed line.
"""

            provider, model = self.llm_client.get_model_for_role("reviewer")
            if provider and model:
                fix_response = ""
                async for chunk in self.llm_client.stream_chat(provider, model, prompt, "reviewer"):
                    fix_response += chunk

                fix_response = fix_response.strip()

                if fix_response:
                    current_content = await self._get_file_content(issue.file_path)
                    if current_content and issue.line_number:
                        lines = current_content.split('\n')
                        if issue.line_number <= len(lines):
                            lines[issue.line_number - 1] = fix_response

                            return {
                                "new_content": '\n'.join(lines),
                                "fix_description": f"Fixed syntax error on line {issue.line_number}"
                            }

        except Exception as e:
            self.log("error", f"Failed to generate syntax fix: {e}")

        return None

    async def _fix_missing_error_handling(self, issue: IntegrationIssue) -> Optional[Dict[str, Any]]:
        """Add basic error handling to main.py."""
        current_content = await self._get_file_content(issue.file_path)
        if not current_content:
            return None

        try:
            # Wrap main logic in try-except
            lines = current_content.split('\n')

            # Find main execution block
            main_start = -1
            for i, line in enumerate(lines):
                if 'if __name__ == "__main__"' in line:
                    main_start = i
                    break

            if main_start >= 0:
                # Insert try block after the if statement
                indent = "    "
                lines.insert(main_start + 1, f"{indent}try:")

                # Indent existing main code
                for i in range(main_start + 2, len(lines)):
                    if lines[i].strip():
                        lines[i] = f"{indent}{lines[i]}"

                # Add except block
                lines.append(f"{indent}except Exception as e:")
                lines.append(f"{indent}    print(f'Application error: {{e}}')")
                lines.append(f"{indent}    import traceback")
                lines.append(f"{indent}    traceback.print_exc()")

                return {
                    "new_content": '\n'.join(lines),
                    "fix_description": "Added error handling to main.py"
                }

        except Exception as e:
            self.log("error", f"Failed to add error handling: {e}")

        return None

    async def _perform_error_focused_analysis(self, error_report: str):
        """Perform targeted analysis based on execution error."""
        try:
            self.log("info", "ðŸŽ¯ Starting error-focused integration analysis...")

            error_context = self._parse_execution_error(error_report)
            if not error_context:
                return

            # Create targeted issue
            issue = IntegrationIssue(
                issue_type="execution_error",
                severity="critical",
                file_path=error_context.get("file_path", ""),
                line_number=error_context.get("line_number"),
                description=f"Execution error: {error_context.get('error_type', 'Unknown')}",
                context=error_context,
                fix_confidence=0.6
            )

            self.integration_issues.append(issue)

            # Try to fix if high confidence
            if self.get_config_value("auto_fix_enabled", True):
                await self._apply_comprehensive_fixes([issue])

        except Exception as e:
            self.log("error", f"Failed to perform error-focused analysis: {e}")

    def _parse_execution_error(self, error_report: str) -> Optional[Dict[str, Any]]:
        """Parse execution error to extract actionable information."""
        try:
            # Extract file and line information
            file_matches = re.findall(r'File "([^"]+)", line (\d+)', error_report)

            # Extract error type
            error_type_match = re.search(r'(\w+Error|\w+Exception):', error_report)
            error_type = error_type_match.group(1) if error_type_match else "Unknown"

            if file_matches:
                file_path, line_num = file_matches[-1]

                # Filter to project files only
                if self.project_manager and self.project_manager.active_project_path:
                    project_root = self.project_manager.active_project_path
                    try:
                        abs_file_path = Path(file_path).resolve()
                        rel_path = abs_file_path.relative_to(project_root)

                        return {
                            "file_path": str(rel_path).replace('\\', '/'),
                            "line_number": int(line_num),
                            "error_type": error_type,
                            "error_report": error_report
                        }
                    except ValueError:
                        # File is outside project
                        pass

        except Exception as e:
            self.log("warning", f"Could not parse execution error: {e}")

        return None

    def _reset_analysis_state(self):
        """Reset all analysis state."""
        self.code_elements.clear()
        self.integration_issues.clear()
        self.import_map.clear()
        self.definition_map.clear()
        self.usage_map.clear()
        self.fixes_applied.clear()

    async def _generate_integration_report(self):
        """Generate comprehensive integration report."""
        total_issues = len(self.integration_issues)
        total_fixes = len(self.fixes_applied)

        # Categorize issues by type and severity
        issue_breakdown = defaultdict(lambda: defaultdict(int))
        for issue in self.integration_issues:
            issue_breakdown[issue.issue_type][issue.severity] += 1

        self.log("info", f"ðŸ“Š Integration Analysis Report:")
        self.log("info", f"   Total Issues Found: {total_issues}")
        self.log("info", f"   Total Fixes Applied: {total_fixes}")
        self.log("info", f"   Success Rate: {(total_fixes / max(total_issues, 1)) * 100:.1f}%")
        self.log("info", f"   Code Elements Analyzed: {len(self.code_elements)}")

        if issue_breakdown:
            self.log("info", "   Issue Breakdown:")
            for issue_type, severities in issue_breakdown.items():
                severity_str = ", ".join(f"{sev}: {count}" for sev, count in severities.items())
                self.log("info", f"     {issue_type}: {severity_str}")

    def get_status_info(self) -> Dict[str, Any]:
        """Get comprehensive plugin status information."""
        return {
            "integration_active": self.state == PluginState.STARTED,
            "is_analyzing": self.is_analyzing,
            "service_manager_available": self.service_manager is not None,
            "project_manager_available": self.project_manager is not None,
            "llm_client_available": self.llm_client is not None,
            "statistics": {
                "code_elements": len(self.code_elements),
                "integration_issues": len(self.integration_issues),
                "fixes_applied": len(self.fixes_applied),
                "pending_requests": self.integration_queue.qsize(),
                "definitions_mapped": len(self.definition_map),
                "files_with_imports": len(self.import_map)
            },
            "last_integration_check": self.last_integration_check.isoformat() if self.last_integration_check else None,
            "configuration": {
                "auto_integration_check": self.get_config_value("auto_integration_check", True),
                "auto_fix_enabled": self.get_config_value("auto_fix_enabled", True),
                "fix_confidence_threshold": self.get_config_value("fix_confidence_threshold", 0.8),
                "analysis_depth": self.get_config_value("analysis_depth", "deep")
            }
        }


class ComprehensiveASTVisitor(ast.NodeVisitor):
    """Comprehensive AST visitor for detailed code analysis."""

    def __init__(self, file_path: str):
        self.file_path = file_path
        self.elements: List[CodeElement] = []
        self.name_usage: List[Tuple[str, int]] = []
        self.current_class: Optional[str] = None

    def visit_ClassDef(self, node: ast.ClassDef):
        """Visit class definition."""
        signature = {
            "methods": [],
            "attributes": [],
            "inheritance": [base.id for base in node.bases if isinstance(base, ast.Name)],
            "constructor_args": []
        }

        # Store current class context
        prev_class = self.current_class
        self.current_class = node.name

        # Process class body
        for item in node.body:
            if isinstance(item, ast.FunctionDef):
                method_sig = {
                    "name": item.name,
                    "args": [arg.arg for arg in item.args.args[1:]],  # Skip 'self'
                    "line": item.lineno
                }
                signature["methods"].append(method_sig)

                if item.name == "__init__":
                    signature["constructor_args"] = method_sig["args"]

        element = CodeElement(
            name=node.name,
            element_type="class",
            file_path=self.file_path,
            line_number=node.lineno,
            signature=signature,
            dependencies=set()
        )

        self.elements.append(element)

        # Continue visiting
        self.generic_visit(node)

        # Restore class context
        self.current_class = prev_class

    def visit_FunctionDef(self, node: ast.FunctionDef):
        """Visit function definition."""
        # Skip methods (handled in class processing)
        if self.current_class is None:
            signature = {
                "args": [arg.arg for arg in node.args.args],
                "defaults": len(node.args.defaults),
                "is_async": isinstance(node, ast.AsyncFunctionDef),
                "returns": self._extract_return_annotation(node)
            }

            element = CodeElement(
                name=node.name,
                element_type="function",
                file_path=self.file_path,
                line_number=node.lineno,
                signature=signature,
                dependencies=set()
            )

            self.elements.append(element)

        self.generic_visit(node)

    def visit_Name(self, node: ast.Name):
        """Visit name usage."""
        if isinstance(node.ctx, ast.Load):
            self.name_usage.append((node.id, node.lineno))
        self.generic_visit(node)

    def _extract_return_annotation(self, node: ast.FunctionDef) -> Optional[str]:
        """Extract return type annotation."""
        if node.returns:
            if isinstance(node.returns, ast.Name):
                return node.returns.id
            elif isinstance(node.returns, ast.Constant):
                return str(node.returns.value)
        return None